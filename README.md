# Towards Lightweight Hyperspectral Image Super-Resolution with Depthwise Separable Convolutional Network


Deep neural networks have demonstrated highly competitive performance in super-resolution (SR) for natural images by learning mappings from low-resolution (LR) to high-resolution (HR) images. However, hyperspectral SR remains an ill-posed problem due to its high spectral dimensionality. Existing methods often rely on large models with a high number of parameters or require the fusion of panchromatic or RGB images, both of which are often impractical in real-world scenarios. Inspired by the MobileNet architecture, we introduce a lightweight depthwise separable dilated convolutional network (DSDCN) to address aforementioned challenges. Specifically, our model leverages multiple depthwise separable convolutions, similar to the MobileNet architecture, and further incorporates a dilated convolution fusion block to make the model more flexible for the extraction of both spatial and spectral features. In addition, we propose a custom loss function that combines mean squared error (MSE), an L2 regularization-based constraint, and a spectral angle-based loss, ensuring high-fidelity spectral and spatial reconstruction. The proposed architecture achieves very competitive performance on two publicly available hyperspectral datasets, making it well-suited for hyperspectral image super-resolution tasks.
