{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b1f033-8c83-4d1a-99da-6bad6f573b1f",
   "metadata": {},
   "source": [
    "##  Load pretrained model and Evaluate the performance for PaviaU_4x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ff7385f-b75d-402f-aa41-19ee4dae8b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 17:59:05.216575: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-08 17:59:05.231146: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746716345.247470 3322220 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746716345.252649 3322220 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-08 17:59:05.269786: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "import tensorflow as tf\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "739d0409-89a4-48fa-baa8-af77bbe93aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746716349.480026 3322220 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31141 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "# Re-define loss components\n",
    "def spectral_angle_loss(y_true, y_pred):\n",
    "    y_true = tf.nn.l2_normalize(y_true, axis=-1)\n",
    "    y_pred = tf.nn.l2_normalize(y_pred, axis=-1)\n",
    "    dot_product = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "    angle = tf.acos(tf.clip_by_value(dot_product, -1.0, 1.0))\n",
    "    return tf.reduce_mean(angle)\n",
    "\n",
    "def l2_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "def combined_loss(lambda_sam=0.1, lambda_l2=0.1):\n",
    "    mse = MeanSquaredError()\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        return mse(y_true, y_pred) + lambda_sam * spectral_angle_loss(y_true, y_pred) + lambda_l2 * l2_loss(y_true, y_pred)\n",
    "    return loss_fn\n",
    "\n",
    "# Load with custom_objects\n",
    "model = load_model(\n",
    "    'PaviaU_4x_saved_model.keras',\n",
    "    custom_objects={'loss_fn': combined_loss(lambda_sam=0.1, lambda_l2=0.1)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e72e42c2-02de-4cac-8f37-158f322e7901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperspectral image shape: (610, 340, 103)\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "\n",
    "# -----------------------------\n",
    "# Load PaviaU Dataset\n",
    "# -----------------------------\n",
    "data = loadmat(\"PaviaU.mat\")\n",
    "if 'paviaU' not in data:\n",
    "    raise KeyError(\"'paviaU' not found in the .mat file.\")\n",
    "hyperspectral_image = data['paviaU'].astype(np.float32)\n",
    "print(\"Hyperspectral image shape:\", hyperspectral_image.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# Parameters\n",
    "# -----------------------------\n",
    "patch_size = (144, 144)\n",
    "downscale_factor = 4\n",
    "nodata_value = -1\n",
    "group_size = 32\n",
    "overlap_size = 8\n",
    "validation_ratio = 0.1\n",
    "test_coords = (0, 0)  # Fixed test patch top-left corner\n",
    "\n",
    "# -----------------------------\n",
    "# Test Patch Location\n",
    "# -----------------------------\n",
    "i0, j0 = test_coords\n",
    "patch_h, patch_w = patch_size\n",
    "\n",
    "# -----------------------------\n",
    "# Extract HR/LR patches spatially\n",
    "# -----------------------------\n",
    "def extract_patches(data, patch_size, downscale_factor, nodata_value, test_coords):\n",
    "    h, w, b = data.shape\n",
    "    i0, j0 = test_coords\n",
    "\n",
    "    train_hr, train_lr = [], []\n",
    "    test_hr, test_lr = None, None\n",
    "\n",
    "    for i in range(0, h - patch_size[0] + 1, patch_size[0]):\n",
    "        for j in range(0, w - patch_size[1] + 1, patch_size[1]):\n",
    "            patch = data[i:i + patch_size[0], j:j + patch_size[1], :]\n",
    "            if patch.shape[:2] != patch_size or np.any(patch == nodata_value):\n",
    "                continue\n",
    "            patch_lr = tf.image.resize(patch,\n",
    "                                       size=(patch_size[0] // downscale_factor, patch_size[1] // downscale_factor),\n",
    "                                       method='area').numpy()\n",
    "            if (i, j) == test_coords:\n",
    "                test_hr = patch[np.newaxis, ...]\n",
    "                test_lr = patch_lr[np.newaxis, ...]\n",
    "            else:\n",
    "                train_hr.append(patch)\n",
    "                train_lr.append(patch_lr)\n",
    "\n",
    "    return (\n",
    "        np.array(train_hr),\n",
    "        np.array(train_lr),\n",
    "        test_hr,\n",
    "        test_lr\n",
    "    )\n",
    "\n",
    "train_hr_full, train_lr_full, test_hr_full, test_lr_full = extract_patches(\n",
    "    hyperspectral_image, patch_size, downscale_factor, nodata_value, test_coords\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Band grouping AFTER train/test split\n",
    "# -----------------------------\n",
    "def apply_band_grouping(patches, group_size, overlap_size):\n",
    "    grouped = []\n",
    "    for patch in patches:\n",
    "        h, w, bands = patch.shape\n",
    "        step = group_size - overlap_size\n",
    "        for g in range(0, bands - group_size + 1, step):\n",
    "            grouped_patch = patch[:, :, g:g + group_size]\n",
    "            grouped.append(grouped_patch)\n",
    "    return np.array(grouped)\n",
    "\n",
    "X_train_hr = apply_band_grouping(train_hr_full, group_size, overlap_size)\n",
    "X_train_lr = apply_band_grouping(train_lr_full, group_size, overlap_size)\n",
    "X_test_hr = apply_band_grouping(test_hr_full, group_size, overlap_size)\n",
    "X_test_lr = apply_band_grouping(test_lr_full, group_size, overlap_size)\n",
    "\n",
    "# -----------------------------\n",
    "# Split 10% of training into validation\n",
    "# -----------------------------\n",
    "num_train = X_train_hr.shape[0]\n",
    "val_size = max(1, int(validation_ratio * num_train))  \n",
    "indices = np.arange(num_train)\n",
    "np.random.shuffle(indices)\n",
    "X_train_hr = X_train_hr[indices]\n",
    "X_train_lr = X_train_lr[indices]\n",
    "X_validation_hr = X_train_hr[:val_size]\n",
    "X_validation_lr = X_train_lr[:val_size]\n",
    "X_train_hr = X_train_hr[val_size:]\n",
    "X_train_lr = X_train_lr[val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa7c7650-d6ce-493c-8fc5-1e160a3bbd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1746716355.808266 3322306 service.cc:148] XLA service 0x7f77bc010af0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1746716355.808285 3322306 service.cc:156]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2025-05-08 17:59:15.829413: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1746716355.903311 3322306 cuda_dnn.cc:529] Loaded cuDNN version 90501\n",
      "E0000 00:00:1746716356.909218 3322306 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1746716357.167182 3322306 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746716357.979231 3322306 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR on the test set: 30.524328003327053\n",
      "Average SSIM on the test set: 0.795899\n",
      "Average SAM on the test set (in degrees): 4.807406022311849\n",
      "Average Correlation Coefficient on the test set: 0.9004752551566085\n",
      "Average ERGAS on the test set: 6.117180203378872\n",
      "Average RMSE: 0.029988304\n"
     ]
    }
   ],
   "source": [
    "def psnr(y_true, y_pred, max_pixel=None):\n",
    "    \"\"\"\n",
    "    Compute PSNR for each spectral band separately and return the average.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth image, shape (H, W, B)\n",
    "        y_pred: Super-resolved image, shape (H, W, B)\n",
    "        max_pixel: Maximum pixel value (None = use actual max from y_true)\n",
    "    \n",
    "    Returns:\n",
    "        Average PSNR across all bands\n",
    "    \"\"\"\n",
    "    if max_pixel is None:\n",
    "        max_pixel = np.max(y_true)  # Auto-detect max value if not provided\n",
    "\n",
    "    B = y_true.shape[-1]  # Number of spectral bands\n",
    "    psnr_values = []\n",
    "    \n",
    "    for i in range(B):  # Loop over bands\n",
    "        mse = np.mean((y_true[..., i] - y_pred[..., i]) ** 2)\n",
    "        if mse == 0:\n",
    "            psnr_values.append(float('inf'))  # Perfect reconstruction\n",
    "        else:\n",
    "            psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
    "            psnr_values.append(psnr)\n",
    "    \n",
    "    return np.mean(psnr_values)  # Average across bands\n",
    "\n",
    "# Function to calculate SSIM with channel_axis\n",
    "def ssim_value(y_true, y_pred):\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f\"Shape mismatch: y_true shape {y_true.shape} vs y_pred shape {y_pred.shape}\")\n",
    "    \n",
    "    data_range = y_true.max() - y_true.min()  # Calculate data range from y_true\n",
    "    ssim_val = ssim(y_true, y_pred, data_range=data_range, channel_axis=-1)\n",
    "    return ssim_val\n",
    "\n",
    "# Function to calculate Correlation Coefficient\n",
    "def correlation_coefficient(y_true, y_pred):\n",
    "    y_true_flat = y_true.flatten()\n",
    "    y_pred_flat = y_pred.flatten()\n",
    "    corr_matrix = np.corrcoef(y_true_flat, y_pred_flat)\n",
    "    corr_value = corr_matrix[0, 1]\n",
    "    return corr_value\n",
    "\n",
    "# Function to calculate Spectral Angle Mapper (SAM) in degrees\n",
    "def sam(y_true, y_pred):\n",
    "    y_true_reshaped = y_true.reshape(-1, y_true.shape[-1])\n",
    "    y_pred_reshaped = y_pred.reshape(-1, y_pred.shape[-1])\n",
    "    \n",
    "    non_zero_mask = (np.linalg.norm(y_true_reshaped, axis=1) > 1e-10) & (np.linalg.norm(y_pred_reshaped, axis=1) > 1e-10)\n",
    "    dot_product = np.sum(y_true_reshaped[non_zero_mask] * y_pred_reshaped[non_zero_mask], axis=1)\n",
    "    norm_true = np.linalg.norm(y_true_reshaped[non_zero_mask], axis=1)\n",
    "    norm_pred = np.linalg.norm(y_pred_reshaped[non_zero_mask], axis=1)\n",
    "    \n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        angles = np.arccos(np.clip(dot_product / (norm_true * norm_pred), -1.0, 1.0))\n",
    "    \n",
    "    if angles.size > 0:\n",
    "        sam_value_degrees = np.mean(angles) * (180 / np.pi)\n",
    "    else:\n",
    "        sam_value_degrees = 0\n",
    "    \n",
    "    return sam_value_degrees\n",
    "\n",
    "# Function to normalize the images\n",
    "def normalize(image):\n",
    "    min_val = np.min(image)\n",
    "    max_val = np.max(image)\n",
    "    return (image - min_val) / (max_val - min_val)  # Normalize to [0, 1]\n",
    "\n",
    "# Function to calculate Root Mean Squared Error (RMSE) for hyperspectral images (normalized)\n",
    "def rmse_bandwise(y_true, y_pred):\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"Shape mismatch between true and predicted images.\")\n",
    "    \n",
    "    bands = y_true.shape[-1]\n",
    "    rmse_per_band = []\n",
    "\n",
    "    for b in range(bands):\n",
    "        band_true = y_true[:, :, b]\n",
    "        band_pred = y_pred[:, :, b]\n",
    "        \n",
    "        mse_band = np.mean((band_true - band_pred) ** 2)\n",
    "        rmse_band_value = np.sqrt(mse_band)\n",
    "        rmse_per_band.append(rmse_band_value)\n",
    "\n",
    "    # Normalize RMSE by the maximum value in y_true across all bands\n",
    "    max_value = np.max(y_true)\n",
    "    normalized_rmse = np.mean(rmse_per_band) / max_value\n",
    "    return normalized_rmse\n",
    "\n",
    "# Function to calculate ERGAS\n",
    "def ergas(y_true, y_pred, scale):\n",
    "    bands = y_true.shape[-1]\n",
    "    ergas_value = 0\n",
    "    \n",
    "    for b in range(bands):\n",
    "        band_true = y_true[:, :, b]\n",
    "        band_pred = y_pred[:, :, b]\n",
    "        mean_band_true = np.mean(band_true)\n",
    "        \n",
    "        # Calculate RMSE for the band without using a separate function\n",
    "        mse_band = np.mean((band_true - band_pred) ** 2)  # Mean Squared Error for the band\n",
    "        rmse_band = np.sqrt(mse_band)  # Root Mean Squared Error for the band\n",
    "        \n",
    "        ergas_value += (rmse_band / mean_band_true) ** 2\n",
    "    \n",
    "    ergas_value = 100 * (1 / scale) * np.sqrt(ergas_value / bands)\n",
    "    return ergas_value\n",
    "\n",
    "# Assuming hybrid_sr_model is trained, and X_test_lr, X_test_hr are defined\n",
    "predicted_hr_images =  model.predict(X_test_lr)\n",
    "\n",
    "downscale_factor = 4 # ERGAS downscale factor\n",
    "\n",
    "# Validate shapes match for test and predictions\n",
    "if predicted_hr_images.shape != X_test_hr.shape:\n",
    "    raise ValueError(f\"Shape mismatch: predicted_hr_images shape {predicted_hr_images.shape} vs X_test_hr shape {X_test_hr.shape}\")\n",
    "\n",
    "# Calculate metrics per test sample\n",
    "psnr_values, ssim_values, cc_values, sam_values, ergas_values, rmse_values = [], [], [], [], [], []\n",
    "\n",
    "for i in range(len(X_test_hr)):\n",
    "    psnr_values.append(psnr(X_test_hr[i], predicted_hr_images[i]))\n",
    "    ssim_values.append(ssim_value(X_test_hr[i], predicted_hr_images[i]))\n",
    "    cc_values.append(correlation_coefficient(X_test_hr[i], predicted_hr_images[i]))\n",
    "    sam_values.append(sam(X_test_hr[i], predicted_hr_images[i]))\n",
    "    ergas_values.append(ergas(X_test_hr[i], predicted_hr_images[i], downscale_factor))\n",
    "    rmse_values.append(rmse_bandwise(X_test_hr[i], predicted_hr_images[i]))\n",
    "\n",
    "# Average metrics\n",
    "average_psnr = np.mean(psnr_values)\n",
    "average_ssim = np.mean(ssim_values)\n",
    "average_cc = np.mean(cc_values)\n",
    "average_sam = np.mean(sam_values)\n",
    "average_ergas = np.mean(ergas_values)\n",
    "average_rmse = np.mean(rmse_values)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Average PSNR on the test set:\", average_psnr)\n",
    "print(\"Average SSIM on the test set:\", average_ssim)\n",
    "print(\"Average SAM on the test set (in degrees):\", average_sam)\n",
    "print(\"Average Correlation Coefficient on the test set:\", average_cc)\n",
    "print(\"Average ERGAS on the test set:\", average_ergas)\n",
    "print(\"Average RMSE:\", average_rmse)  # Indicate RMSE is normalized"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
